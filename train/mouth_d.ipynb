{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1UdQyVPD4ogVD1rcBb2zVhvIrBmfYPVWP","authorship_tag":"ABX9TyPitpBK3nIqwa3bL/wfIELo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p97B6CNhwN8w","executionInfo":{"status":"ok","timestamp":1743095444063,"user_tz":-330,"elapsed":718823,"user":{"displayName":"Major Project","userId":"13217487735143892560"}},"outputId":"27e10cb1-0940-473c-bf3b-03034bac821e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1159 images belonging to 2 classes.\n","Found 289 images belonging to 2 classes.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 16s/step - accuracy: 0.5488 - loss: 0.7850 - val_accuracy: 0.3668 - val_loss: 0.7772\n","Epoch 2/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 215ms/step - accuracy: 0.7340 - loss: 0.5287 - val_accuracy: 0.4533 - val_loss: 0.9148\n","Epoch 3/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 192ms/step - accuracy: 0.7992 - loss: 0.4355 - val_accuracy: 0.3495 - val_loss: 1.3378\n","Epoch 4/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 219ms/step - accuracy: 0.8236 - loss: 0.3742 - val_accuracy: 0.3668 - val_loss: 1.7748\n","Epoch 5/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 202ms/step - accuracy: 0.8717 - loss: 0.2988 - val_accuracy: 0.3253 - val_loss: 2.2076\n","Epoch 6/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 221ms/step - accuracy: 0.9121 - loss: 0.2274 - val_accuracy: 0.2907 - val_loss: 2.4907\n","Epoch 7/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 207ms/step - accuracy: 0.9339 - loss: 0.1731 - val_accuracy: 0.3356 - val_loss: 2.2929\n","Epoch 8/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 212ms/step - accuracy: 0.9353 - loss: 0.1742 - val_accuracy: 0.3702 - val_loss: 3.1042\n","Epoch 9/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 252ms/step - accuracy: 0.9511 - loss: 0.1296 - val_accuracy: 0.3149 - val_loss: 3.4684\n","Epoch 10/10\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.9676 - loss: 0.1003 - val_accuracy: 0.4325 - val_loss: 2.9319\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Model training complete and saved as lips_detection_model.h5\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","# Define dataset path\n","data_dir = \"/content/drive/MyDrive/DrowsiScan/train/data/lips\"\n","img_size = (64, 64)\n","batch_size = 32\n","epochs = 10  # Adjust based on results\n","\n","# Data Augmentation & Preprocessing\n","datagen = ImageDataGenerator(\n","    rescale=1.0/255.0,\n","    validation_split=0.2\n",")\n","\n","\n","train_generator = datagen.flow_from_directory(\n","    \"/content/drive/MyDrive/DrowsiScan/train/data/lips\", # Pass directory path as positional argument\n","    target_size=(64,64),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='training',\n","    shuffle=True  # Ensures proper train/val split\n",")\n","\n","val_generator = datagen.flow_from_directory(\n","    \"/content/drive/MyDrive/DrowsiScan/train/data/lips\" , # Pass directory path as positional argument\n","    target_size=(64,64),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='validation',\n","    shuffle=True\n",")\n","\n","\n","# Define CNN Model\n","model = Sequential([\n","    Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),\n","    MaxPooling2D(2,2),\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(\n","    train_generator,\n","    validation_data=val_generator,\n","    epochs=epochs\n",")\n","\n","# Save the trained model\n","model.save(\"lips_detection_model.h5\")\n","print(\"Model training complete and saved as lips_detection_model.h5\")"]}]}